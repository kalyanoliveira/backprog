{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from graphviz import Digraph\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Value Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0 \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, \\\"{self.label}\\\")\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mathematical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(1.0); a.label=\"a\"\n",
    "b = Value(2.0); b.label=\"b\"\n",
    "\n",
    "c = a + b; c.label=\"c\"\n",
    "\n",
    "d = Value(-3.0); d.label=\"d\"\n",
    "\n",
    "e = c * d; e.label=\"e\"\n",
    "\n",
    "f = Value(-1.0); f.label = \"f\"\n",
    "\n",
    "o = e + f; o.label = \"o\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Graph of the Mathematical Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of known nodes\n",
    "# and their connections\n",
    "# starting from a \"root\" node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(node):\n",
    "        if node not in nodes:\n",
    "            nodes.add(node)\n",
    "        for prev in node._prev:\n",
    "            edges.add((prev, node))\n",
    "            build(prev)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    # For each Value object, create a node\n",
    "    for node in nodes:\n",
    "        node_id = str(id(node))\n",
    "        dot.node(name=node_id, label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "        # If this Value object was created by an operation,\n",
    "        # create a node for that operation, and connect it\n",
    "        # to the Value object\n",
    "        if node._op:\n",
    "            dot.node(name= node_id + str(node._op), label=node._op)\n",
    "            dot.edge(node_id + node._op, node_id)\n",
    "\n",
    "    # Connect a child node to its parent's operation node\n",
    "    for child, parent in edges:\n",
    "        dot.edge(str(id(child)), str(id(parent)) + parent._op)\n",
    "\n",
    "    return dot\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Manual Backpropagation\n",
    "\n",
    "$o = f + e$\n",
    "\n",
    "$\\frac{do}{do} = 1$\n",
    "\n",
    "Hence, `o.grad = 1.0`\n",
    "\n",
    "$\\frac{do}{df}= \\frac{do}{do} * \\frac{do}{df} = 1 * 1 = 1$\n",
    "\n",
    "=> `f.grad = o.grad * 1.0`\n",
    "\n",
    "$\\frac{do}{de} = \\frac{do}{do} * \\frac{do}{df} = 1 *1 = 1$\n",
    "\n",
    "=> `e.grad = o.grad * 1.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "$e = cd$\n",
    "\n",
    "$\\frac{do}{dc} = \\frac{do}{de} * \\frac{de}{dc}$ &emsp; *Because of the chain rule*\n",
    "\n",
    "$\\frac{do}{dc} =  1 * d$\n",
    "\n",
    "$\\frac{do}{dc} = d$\n",
    "\n",
    "=> `c.grad = e.grad * d.data`\n",
    "\n",
    "$\\frac{do}{dd} = \\frac{do}{de} * \\frac{de}{dd} = 1 * c = c$\n",
    "\n",
    "=> `d.grad = e.grad * c.data`\n",
    "\n",
    "<br>\n",
    "\n",
    "$c = a + b$\n",
    "\n",
    "$\\frac{do}{da} = \\frac{do}{dc} * \\frac{dc}{da} = d * 1 = d$\n",
    "\n",
    "=> `a.grad = c.grad * 1.0`\n",
    "\n",
    "$\\frac{do}{db} = \\frac{do}{dc} * \\frac{dc}{db} = d * 1 = d$\n",
    "\n",
    "=> `b.grad = c.grad * 1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "f.grad = o.grad * 1.0\n",
    "e.grad = o.grad * 1.0\n",
    "c.grad = e.grad * d.data\n",
    "d.grad = e.grad * c.data\n",
    "a.grad = c.grad * 1.0\n",
    "b.grad = c.grad * 1.0\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another example of manual backpropagation with a neuron-like structure\n",
    "\n",
    "First we create an activation function (tanh) in our Value object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0 \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, \\\"{self.label}\\\")\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        # math.exp(int) does e^(int)\n",
    "        t = (math.exp(2*self.data) - 1)/(math.exp(2*self.data) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the expression of a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "# Weights\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "# Bias\n",
    "b = Value(6.8813735879195432, label=\"b\")\n",
    "\n",
    "# Weighted inputs\n",
    "x1w1 = x1 * w1; x1w1.label=\"x1*w1\"\n",
    "x2w2 = x2 * w2; x2w2.label=\"x2*w2\"\n",
    "\n",
    "# Overall sum\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label=\"x1*w1 + x2*w2\"\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "# Activation function\n",
    "o = n.tanh(); o.label=\"o\"\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we perform the actual backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o = tanh(n)$\n",
    "\n",
    "$\\frac{do}{do}=1$\n",
    "\n",
    "=> `o.grad = 1.0`\n",
    "\n",
    "$\\frac{do}{dn} = \\frac{do}{do} * \\frac{do}{dn} = 1 * \\frac{d}{dn}(tanh(n)) = 1 * (1 - tanh^2(n)) = 1 * (1 - o^2) = 1 - o^2$\n",
    "\n",
    "=> `n.grad = o.grad * (1 - o.data**2)`\n",
    "\n",
    "<br>\n",
    "\n",
    "$n = x1w1x2w2 + b$\n",
    "\n",
    "$\\frac{do}{dx1w1x2w2} = \\frac{do}{dn} * \\frac{dn}{dx1w1x2w2} = (1 - o^2) * 1 = 1 - o^2$\n",
    "\n",
    "=> `x1w1x2w2.grad = n.grad * 1.0`\n",
    "\n",
    "$\\frac{do}{db} = \\frac{do}{dn} * \\frac{dn}{db} = (1 - o^2) * 1 = 1 - o^2$\n",
    "\n",
    "=> `b.grad = n.grad * 1.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "$x1w1x2w2 = x1w1 + x2w2$\n",
    "\n",
    "$\\frac{do}{dx1w1} = \\frac{do}{dx1w1x2w2} * \\frac{dx1w1x2w2}{dx1w1} = (1 - o^2) * 1 = 1 - o^2$\n",
    "\n",
    "=> `x1w1.grad = x1w1x2w2.grad * 1.0`\n",
    "\n",
    "$\\frac{do}{x2w2} = \\frac{do}{dx1w1x2w2} * \\frac{dx1w1x2w2}{x2w2} = (1 - o^2) * 1 = 1 - o^2$\n",
    "\n",
    "=> `x2w2.grad = x1w1x2w2.grad * 1.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "$x1w1 = x1 * w1$\n",
    "\n",
    "$\\frac{do}{dx1} = \\frac{do}{dx1w1} * \\frac{dx1w1}{dx1} = (1 - o^2) * w1$\n",
    "\n",
    "=> `x1.grad = x1w1.grad * w1.data`\n",
    "\n",
    "$\\frac{do}{w1} = \\frac{do}{dx1w1} * \\frac{dx1w1}{dw1} = (1 - o^2) * x1$\n",
    "\n",
    "=> `w1.grad = x1w1.grad * x1.data`\n",
    "\n",
    "<br>\n",
    "\n",
    "$x2w2 = x2 * w2$\n",
    "\n",
    "$\\frac{do}{dx2} = \\frac{do}{dx2w2} * \\frac{dx2w2}{dx2} = (1 - o^2) * w2$\n",
    "\n",
    "=> `x2.grad = x2w2.grad * w2.data`\n",
    "\n",
    "$\\frac{do}{dw2} = \\frac{do}{dx2w2} * \\frac{dx2w2}{dw2} = (1 - o^2) * x2$\n",
    "\n",
    "=> `w2.grad = x2w2.grad * x2.data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "n.grad = o.grad * (1 - o.data**2)\n",
    "x1w1x2w2.grad = n.grad * 1.0\n",
    "b.grad = n.grad * 1.0\n",
    "x1w1.grad = x1w1x2w2.grad * 1.0\n",
    "x2w2.grad = x1w1x2w2.grad * 1.0\n",
    "x1.grad = x1w1.grad * w1.data\n",
    "w1.grad = x1w1.grad * x1.data\n",
    "x2.grad = x2w2.grad * w2.data\n",
    "w2.grad = x2w2.grad * x2.data\n",
    "draw_dot(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's actually start trying to automate this\n",
    "\n",
    "For every parent Value object, I want to be able to call a function called \"_backward\"\n",
    "\n",
    "This \"_backward\" function simply computes the gradients of the children nodes\n",
    "\n",
    "If a node has no children nodes, its \"_backward\" function will simply do nothing\n",
    "\n",
    "I'll add comments in the changed \\_\\_add\\_\\_ method to make this more clear\n",
    "\n",
    "I will also add the create graph functionality inside the value object, such that we can call Value.create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "\n",
    "    # Supported operations\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        # Create the _backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself and dout/dother, and set the grads of self and other\n",
    "            dout_dself = 1.0\n",
    "            dout_dother = 1.0\n",
    "            self.grad = out.grad * dout_dself # Because of the chain rule\n",
    "            other.grad = out.grad * dout_dother\n",
    "\n",
    "        # Set the _backward behavior of \"out\" to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        # Create the behavior we expect when calling backward\n",
    "        def _backward():\n",
    "            # Compute the local derivatives, and set the gradients of the children nodes\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "            self.grad = out.grad * dout_dself\n",
    "            other.grad = out.grad * dout_dother\n",
    "        \n",
    "        # Set the \"_backward\" behavior of out to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1)/(math.exp(2*self.data) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "\n",
    "        # Define the backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself, and then set self.grad\n",
    "            # d/dx(tanh(x)) = 1 - tanh^2(x)\n",
    "            # out = tanh(self)\n",
    "            dout_dself = 1 - out.data**2\n",
    "            self.grad = out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "        # Set the backward behavior of out to what its supposed to be\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Cosmetic stuff\n",
    "    def __repr__(self):\n",
    "        return f\"Value(\\\"{self.label}\\\", data={self.data:.4f})\"\n",
    "    \n",
    "    def create_graph(self):\n",
    "        # Create a set of known nodes and edges\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for prev in node._prev:\n",
    "                        edges.add((prev, node))\n",
    "                        build(prev)\n",
    "\n",
    "            build(self)\n",
    "\n",
    "            return nodes, edges\n",
    "        \n",
    "        def draw_dot(root):\n",
    "            dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            # For each Value object, create a node\n",
    "            for node in nodes:\n",
    "                dot.node(name=str(id(node)), label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "                # If this Value object was created by an operation,\n",
    "                # create a node for that, and connect it to the \n",
    "                # this Value object\n",
    "                if node._op:\n",
    "                    dot.node(name=str(id(node)) + node._op, label=node._op)\n",
    "                    dot.edge(str(id(node)) + node._op, str(id(node)))\n",
    "\n",
    "            # For each known edge, create an edge\n",
    "            for child, parent in edges:\n",
    "                dot.edge(str(id(child)), str(id(parent)) + parent._op)\n",
    "\n",
    "            return dot\n",
    "        \n",
    "        return draw_dot(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we re-run our expression (I think this is what is referred to as the forward pass, or something like that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "# Weights\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "# Bias\n",
    "b = Value(6.8813735879195432, label=\"b\")\n",
    "\n",
    "# Weighted inputs\n",
    "x1w1 = x1 * w1; x1w1.label=\"x1w1\"\n",
    "x2w2 = x2 * w2; x2w2.label=\"x2w2\"\n",
    "\n",
    "# Overall sum\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label=\"x1w1x2w2\"\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "# Activation function\n",
    "o = n.tanh(); o.label=\"o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we call the backward of each node, making sure it's in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "\n",
    "o._backward()\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w2._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()\n",
    "x1._backward()\n",
    "w1._backward()\n",
    "x2._backward()\n",
    "w2._backward()\n",
    "\n",
    "o.create_graph() # Just make sure to call this last, else Jupyter will not display the graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is still not fully ideal, as for any root node, I want to simply be able to call a \"backward\" function that does all of the \"_backward\" calls for me, in the right order\n",
    "\n",
    "So let's do that\n",
    "\n",
    "Take a look at the comments in the backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "\n",
    "    # Gradient computing\n",
    "    def backward(self):\n",
    "        # Treat self as the root node, find out the order of \"_backward\" calls, and perform those calls\n",
    "\n",
    "        # do/do = 1.0\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # Perform something similar to topological sort\n",
    "        # \"Sort every node of a directed graph,\n",
    "        # such that every node appears before\n",
    "        # all of the nodes it points to\"\n",
    "        \n",
    "        # This is useful\n",
    "        # Our directed graph is simply the graph of our Neuron expression\n",
    "        # We want the first nodes of a sorting of our Neuron Value objects to be the inputs and weights,\n",
    "        # followed by the weighted inputs\n",
    "        # followed by the sum of the weighted inputs, and the bias\n",
    "        # followed by the sum of those two, the \"sum of the neuron\"\n",
    "        # followed by the squashed sum of the neuron\n",
    "\n",
    "        # This is kind of like doing a topological sort\n",
    "\n",
    "        topo_sorted = []\n",
    "        seen = set()\n",
    "\n",
    "        def explore(node):\n",
    "            # For each child: \n",
    "            for child in node._prev:\n",
    "                # If I do know you, what should I do?\n",
    "                    # Wait for the exploration of its sibling, it might have a history\n",
    "                # If I don't know you, what should I do?\n",
    "                if child not in seen:\n",
    "                    # Take record of the child, and explore its history\n",
    "                    seen.add(child)\n",
    "                    explore(child)\n",
    "                # Now that everything is explored, we can add this child to our topo sort\n",
    "                topo_sorted.append(child)\n",
    "\n",
    "        explore(self) # Changes the topo_sorted list to what it should be\n",
    "\n",
    "        # Now that we have the sorted Value objects,\n",
    "        # call the \"_backward\" of self,\n",
    "        # and call the \"_backward\" of the Value objects in the reversed of the sorted digraph\n",
    "\n",
    "        # for index, val in enumerate(topo_sorted):\n",
    "        #     print(f\"{index}: {val}\")\n",
    "\n",
    "        self._backward()\n",
    "\n",
    "        for node in reversed(topo_sorted):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "    # Supported operations\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        # Create the _backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself and dout/dother, and set the grads of self and other\n",
    "            dout_dself = 1.0\n",
    "            dout_dother = 1.0\n",
    "            self.grad = out.grad * dout_dself # Because of the chain rule\n",
    "            other.grad = out.grad * dout_dother\n",
    "\n",
    "        # Set the _backward behavior of \"out\" to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        # Create the behavior we expect when calling backward\n",
    "        def _backward():\n",
    "            # Compute the local derivatives, and set the gradients of the children nodes\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "            self.grad = out.grad * dout_dself\n",
    "            other.grad = out.grad * dout_dother\n",
    "        \n",
    "        # Set the \"_backward\" behavior of out to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1)/(math.exp(2*self.data) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "\n",
    "        # Define the backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself, and then set self.grad\n",
    "            # d/dx(tanh(x)) = 1 - tanh^2(x)\n",
    "            # out = tanh(self)\n",
    "            dout_dself = 1 - out.data**2\n",
    "            self.grad = out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "        # Set the backward behavior of out to what its supposed to be\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    # Cosmetic stuff\n",
    "    def __repr__(self):\n",
    "        return f\"Value(\\\"{self.label}\\\", data={self.data:.4f})\"\n",
    "    \n",
    "    def create_graph(self):\n",
    "        # Create a set of known nodes and edges\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for prev in node._prev:\n",
    "                        edges.add((prev, node))\n",
    "                        build(prev)\n",
    "\n",
    "            build(self)\n",
    "\n",
    "            return nodes, edges\n",
    "        \n",
    "        def draw_dot(root):\n",
    "            dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            # For each Value object, create a node\n",
    "            for node in nodes:\n",
    "                dot.node(name=str(id(node)), label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "                # If this Value object was created by an operation,\n",
    "                # create a node for that, and connect it to the \n",
    "                # this Value object\n",
    "                if node._op:\n",
    "                    dot.node(name=str(id(node)) + node._op, label=node._op)\n",
    "                    dot.edge(str(id(node)) + node._op, str(id(node)))\n",
    "\n",
    "            # For each known edge, create an edge\n",
    "            for child, parent in edges:\n",
    "                dot.edge(str(id(child)), str(id(parent)) + parent._op)\n",
    "\n",
    "            return dot\n",
    "        \n",
    "        return draw_dot(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "# Weights\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "# Bias\n",
    "b = Value(6.8813735879195432, label=\"b\")\n",
    "\n",
    "# Weighted inputs\n",
    "x1w1 = x1 * w1; x1w1.label=\"x1w1\"\n",
    "x2w2 = x2 * w2; x2w2.label=\"x2w2\"\n",
    "\n",
    "# Overall sum\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label=\"x1w1x2w2\"\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "# Activation function\n",
    "o = n.tanh(); o.label=\"o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()\n",
    "o.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem with the Value object we created so far exists when we try to do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label=\"a\")\n",
    "b = a + a; b.label=\"b\"\n",
    "\n",
    "b.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, $b = a + a$, which is equal to $b = 2a$\n",
    "\n",
    "Hence, $\\frac{db}{da} = 2$\n",
    "\n",
    "But, from the above, we see that `a.grad = 1.0`\n",
    "\n",
    "This bug arises from the fact that when `b.backward()` is called, `b._backward()` is also called\n",
    "\n",
    "`b._backward()` does `self.grad = out.grad * dout_dself`, and it does `other.grad = out.grad * dout_dother`\n",
    "\n",
    "But both `self` and `other` are `a`\n",
    "\n",
    "So we set `a.grad = 1.0` once with `self.grad`, and twice with `other.grad`\n",
    "\n",
    "To fix this, we simply need to change the `=` in `self.grad = ...` to a `+=`\n",
    "\n",
    "and the same thing for `other.grad = ...`\n",
    "\n",
    "We'll basically do this for every operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "\n",
    "    # Gradient computing\n",
    "    def backward(self):\n",
    "        # Treat self as the root node, find out the order of \"_backward\" calls, and perform those calls\n",
    "\n",
    "        # do/do = 1.0\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # Perform something similar to topological sort\n",
    "        # \"Sort every node of a directed graph,\n",
    "        # such that every node appears before\n",
    "        # all of the nodes it points to\"\n",
    "        \n",
    "        # This is useful\n",
    "        # Our directed graph is simply the graph of our Neuron expression\n",
    "        # We want the first nodes of a sorting of our Neuron Value objects to be the inputs and weights,\n",
    "        # followed by the weighted inputs\n",
    "        # followed by the sum of the weighted inputs, and the bias\n",
    "        # followed by the sum of those two, the \"sum of the neuron\"\n",
    "        # followed by the squashed sum of the neuron\n",
    "\n",
    "        # This is kind of like doing a topological sort\n",
    "\n",
    "        topo_sorted = []\n",
    "        seen = set()\n",
    "\n",
    "        def explore(node):\n",
    "            # For each child: \n",
    "            for child in node._prev:\n",
    "                # If I do know you, what should I do?\n",
    "                    # Wait for the exploration of its sibling, it might have a history\n",
    "                # If I don't know you, what should I do?\n",
    "                if child not in seen:\n",
    "                    # Take record of the child, and explore its history\n",
    "                    seen.add(child)\n",
    "                    explore(child)\n",
    "                # Now that everything is explored, we can add this child to our topo sort\n",
    "                topo_sorted.append(child)\n",
    "\n",
    "        explore(self) # Changes the topo_sorted list to what it should be\n",
    "\n",
    "        # Now that we have the sorted Value objects,\n",
    "        # call the \"_backward\" of self,\n",
    "        # and call the \"_backward\" of the Value objects in the reversed of the sorted digraph\n",
    "\n",
    "        # for index, val in enumerate(topo_sorted):\n",
    "        #     print(f\"{index}: {val}\")\n",
    "\n",
    "        self._backward()\n",
    "\n",
    "        for node in reversed(topo_sorted):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "    # Supported operations\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        # Create the _backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself and dout/dother, and set the grads of self and other\n",
    "            dout_dself = 1.0\n",
    "            dout_dother = 1.0\n",
    "            self.grad += out.grad * dout_dself # Because of the chain rule\n",
    "            other.grad += out.grad * dout_dother\n",
    "\n",
    "        # Set the _backward behavior of \"out\" to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        # Create the behavior we expect when calling backward\n",
    "        def _backward():\n",
    "            # Compute the local derivatives, and set the gradients of the children nodes\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "        \n",
    "        # Set the \"_backward\" behavior of out to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1)/(math.exp(2*self.data) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "\n",
    "        # Define the backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself, and then set self.grad\n",
    "            # d/dx(tanh(x)) = 1 - tanh^2(x)\n",
    "            # out = tanh(self)\n",
    "            dout_dself = 1 - out.data**2\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "        # Set the backward behavior of out to what its supposed to be\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    # Cosmetic stuff\n",
    "    def __repr__(self):\n",
    "        return f\"Value(\\\"{self.label}\\\", data={self.data:.4f})\"\n",
    "    \n",
    "    def create_graph(self):\n",
    "        # Create a set of known nodes and edges\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for prev in node._prev:\n",
    "                        edges.add((prev, node))\n",
    "                        build(prev)\n",
    "\n",
    "            build(self)\n",
    "\n",
    "            return nodes, edges\n",
    "        \n",
    "        def draw_dot(root):\n",
    "            dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            # For each Value object, create a node\n",
    "            for node in nodes:\n",
    "                dot.node(name=str(id(node)), label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "                # If this Value object was created by an operation,\n",
    "                # create a node for that, and connect it to the \n",
    "                # this Value object\n",
    "                if node._op:\n",
    "                    dot.node(name=str(id(node)) + node._op, label=node._op)\n",
    "                    dot.edge(str(id(node)) + node._op, str(id(node)))\n",
    "\n",
    "            # For each known edge, create an edge\n",
    "            for child, parent in edges:\n",
    "                dot.edge(str(id(child)), str(id(parent)) + parent._op)\n",
    "\n",
    "            return dot\n",
    "        \n",
    "        return draw_dot(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now backpropagating through `b = a + a` should work, resulting in `a.grad = 2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label=\"a\")\n",
    "b = a + a; b.label=\"b\"\n",
    "\n",
    "b.backward()\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to try and break up `tanh` into its individual operations\n",
    "\n",
    "You see, $tanh(x) = (e^{(2x)} - 1) / (e^{(2x)} - 1)$\n",
    "\n",
    "But we are going to do so by doing the most \"atomic\" operations possible\n",
    "\n",
    "We need to implement Division of Value objects, Exponentiating a Value object to an int/float, Exponentiating \"e\" to a Value object, Multiplication of a Value object by a constant, and Subtraction of Value objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "\n",
    "\n",
    "    # Operations \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        # Create the _backward behavior of out\n",
    "        def _backward():\n",
    "            # Calculate dout/dself and dout/dother, and set the grads of self and other\n",
    "            dout_dself = 1.0\n",
    "            dout_dother = 1.0\n",
    "            self.grad += out.grad * dout_dself # Because of the chain rule\n",
    "            other.grad += out.grad * dout_dother\n",
    "\n",
    "        # Set the _backward behavior of \"out\" to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        # Create the behavior we expect when calling backward\n",
    "        def _backward():\n",
    "            # Compute the local derivatives, and set the gradients of the children nodes\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "        \n",
    "        # Set the \"_backward\" behavior of out to what it's supposed to be\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other \n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data**other, (self,), f\"^{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = other * self.data**(other - 1)\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self):\n",
    "        out = Value(math.exp(self.data), (self, ), \"e^\")\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = out.data\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    # Backprog stuff \n",
    "    def backward(self):\n",
    "        # Treat self as the root node, find out the order of \"_backward\" calls, and perform those calls\n",
    "\n",
    "        # do/do = 1.0\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # Perform something similar to topological sort\n",
    "        # \"Sort every node of a directed graph,\n",
    "        # such that every node appears before\n",
    "        # all of the nodes it points to\"\n",
    "        \n",
    "        # This is useful\n",
    "        # Our directed graph is simply the graph of our Neuron expression\n",
    "        # We want the first nodes of a sorting of our Neuron Value objects to be the inputs and weights,\n",
    "        # followed by the weighted inputs\n",
    "        # followed by the sum of the weighted inputs, and the bias\n",
    "        # followed by the sum of those two, the \"sum of the neuron\"\n",
    "        # followed by the squashed sum of the neuron\n",
    "\n",
    "        # This is kind of like doing a topological sort\n",
    "\n",
    "        topo_sorted = []\n",
    "        seen = set()\n",
    "\n",
    "        def explore(node):\n",
    "            # For each child: \n",
    "            for child in node._prev:\n",
    "                # If I do know you, what should I do?\n",
    "                    # Wait for the exploration of its sibling, it might have a history\n",
    "                # If I don't know you, what should I do?\n",
    "                if child not in seen:\n",
    "                    # Take record of the child, and explore its history\n",
    "                    seen.add(child)\n",
    "                    explore(child)\n",
    "                # Now that everything is explored, we can add this child to our topo sort\n",
    "                    topo_sorted.append(child)\n",
    "\n",
    "\n",
    "        explore(self) # Changes the topo_sorted list to what it should be\n",
    "\n",
    "        # Now that we have the sorted Value objects,\n",
    "        # call the \"_backward\" of self,\n",
    "        # and call the \"_backward\" of the Value objects in the reversed of the sorted digraph\n",
    "\n",
    "        # for index, val in enumerate(topo_sorted):\n",
    "        #     print(f\"{index}: {val}\")\n",
    "\n",
    "        self._backward()\n",
    "\n",
    "        for node in reversed(topo_sorted):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "\n",
    "    # Cosmetic stuff\n",
    "    def __repr__(self):\n",
    "        return f\"Value(\\\"{self.label}\\\", data={self.data:.4f})\"\n",
    "    \n",
    "    def create_graph(self):\n",
    "        # Create a set of known nodes and edges\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for prev in node._prev:\n",
    "                        edges.add((prev, node))\n",
    "                        build(prev)\n",
    "\n",
    "            build(self)\n",
    "\n",
    "            return nodes, edges\n",
    "        \n",
    "        def draw_dot(root):\n",
    "            # graph_attr={'rankdir': 'LR'}\n",
    "            dot = Digraph(format='svg')\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            # For each Value object, create a node\n",
    "            for node in nodes:\n",
    "                dot.node(name=str(id(node)), label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "                # If this Value object was created by an operation,\n",
    "                # create a node for that, and connect it to the \n",
    "                # this Value object\n",
    "                if node._op:\n",
    "                    dot.node(name=str(id(node)) + node._op, label=node._op)\n",
    "                    dot.edge(str(id(node)) + node._op, str(id(node)))\n",
    "\n",
    "            # For each known edge, create an edge\n",
    "            for child, parent in edges:\n",
    "                dot.edge(str(id(child)), str(id(parent)) + parent._op)\n",
    "\n",
    "            return dot\n",
    "        \n",
    "        return draw_dot(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to do a couple of cool stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0, label=\"a\")\n",
    "b = Value(4.0, label=\"b\")\n",
    "\n",
    "c = b / a; c.label=\"c\"\n",
    "\n",
    "d = 2*c; d.label=\"d\"\n",
    "\n",
    "e = d + d\n",
    "\n",
    "# e^(2d), where \"e\" is Euler's number\n",
    "f = e.exp(); f.label=\"f\"\n",
    "\n",
    "f.backward(); f.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we should be able to build out our Neuron expression without using the Value.tanh() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just building out the expression for a Neuron\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label=\"x1*w1\"\n",
    "x2w2 = x2*w2; x2w2.label=\"x2*w2\"\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\n",
    "\n",
    "b = Value(6.8813735870195432, label=\"b\")\n",
    "\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "two_n = 2*n; two_n.label=\"2*n\"\n",
    "\n",
    "e = two_n.exp(); e.label = \"e^(2*n)\"\n",
    "\n",
    "e_minus_1 = e - 1; e_minus_1.label = \"e - 1\"\n",
    "\n",
    "e_plus_1 = e + 1; e_plus_1.label = \"e + 1\"\n",
    "\n",
    "o = e_minus_1 / e_plus_1; o.label = \"o\"\n",
    "\n",
    "o.backward()\n",
    "o.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the reason the code above does not work is due to my bad implementation of a topological sort\n",
    "\n",
    "Turns out that my `explore()` function, which is an inner function of the Value.backward(), is just straight up broken\n",
    "\n",
    "Here's the code for that\n",
    "\n",
    "``` python\n",
    "def explore(node):\n",
    "    # For each child: \n",
    "    for child in node._prev:\n",
    "        # If I do know you, what should I do?\n",
    "            # Wait for the exploration of its sibling, it might have a history\n",
    "        # If I don't know you, what should I do?\n",
    "        if child not in seen:\n",
    "            # Take record of the child, and explore its history\n",
    "            seen.add(child)\n",
    "            explore(child)\n",
    "        # Now that everything is explored, we can add this child to our topo sort\n",
    "        topo_sorted.append(child)\n",
    "```\n",
    "\n",
    "You can see the problem with this kind of approach: we allow ourselves to add nodes that we have potentially already seen\n",
    "\n",
    "For instance, we add the node `e` twice to our list\n",
    "\n",
    "We add it once whilst exploring the children nodes of `e - 1`\n",
    "\n",
    "And we add it a second time whilst exploring the children nodes of `e + 1`\n",
    "\n",
    "Hence, we end up calling `e._backward()` twice, which is not what we want\n",
    "\n",
    "To fix this problem, we simply need to move `topo_sorted.append(child)` to the inner `if` statement\n",
    "\n",
    "Like so: \n",
    "\n",
    "``` python\n",
    "def explore(node):\n",
    "    # For each child: \n",
    "    for child in node._prev:\n",
    "        # If I do know you, what should I do?\n",
    "            # Wait for the exploration of its sibling, it might have a history\n",
    "        # If I don't know you, what should I do?\n",
    "        if child not in seen:\n",
    "            # Take record of the child, and explore its history\n",
    "            seen.add(child)\n",
    "            explore(child)\n",
    "            # Now that everything is explored, we can add this child to our topo sort\n",
    "            topo_sorted.append(child)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'll do now is that change, plus some cleaning up of the Value class\n",
    "\n",
    "I will also change the `explore` algorithm to the below:\n",
    "\n",
    "``` python\n",
    "def explore(node):\n",
    "            if node not in seen:\n",
    "                seen.add(node)\n",
    "                for child in node._prev:\n",
    "                    explore(child)\n",
    "                topo_sorted.append(node)\n",
    "```\n",
    "\n",
    "The previous explore algorithm I showed was a creation of my own, and clearly I made some mistakes\n",
    "\n",
    "This is the \"documented\" explore algorithm, and so it is certain to work\n",
    "\n",
    "I will dwell more on my implementation of this task compared to the \"correct\" implementation later on - who knows, maybe I'll find out that my implementation is completely valid, or that it is wrong in some other manner\n",
    "\n",
    "Further, I will also implement __radd__ and __rsub__, in case we need to do some constant +- Value object operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\", special_label=\"\"):\n",
    "        self.data = data\n",
    "        self.label = label + special_label\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "\n",
    "\n",
    "    # Operations\n",
    "    # Value + Value\n",
    "    def __add__(self, other):\n",
    "        # If other is a Value object, keep it\n",
    "        # Else (if other is just a int/float, for instance), create a Value object based on other\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "    \n",
    "        # Create out's _backward behavior\n",
    "        def _backward():\n",
    "            # Compute the local derivatives of out with respect to self and other\n",
    "            # and change self's and other's grads\n",
    "            dout_dself = 1.0\n",
    "            dout_dother = 1.0\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "\n",
    "        # Set out's _backward behavior to what it should be\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    # Value * Value\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # -Value\n",
    "    def __neg__(self):\n",
    "        # self.__mul__(-1)\n",
    "        return self * -1\n",
    "    \n",
    "    # Value - Value\n",
    "    def __sub__(self, other):\n",
    "        # self.__add__(other.__neg__)\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return self - other\n",
    "\n",
    "    # If python cannot do other * self (for instance, when \"other\" is an int/float),\n",
    "    # then this function gets called\n",
    "    # as in 2 * Value\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    # (Value)^int\n",
    "    def __pow__(self, other):\n",
    "        # Make sure that other is a float or int\n",
    "        assert isinstance(other, (int, float))\n",
    "        \n",
    "        out = Value(self.data**other, (self, ), f\"^{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = other * self.data**(other - 1)\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Value / Value\n",
    "    def __truediv__(self, other):\n",
    "        # self.__mul__(other.__pow__(-1))\n",
    "        return self * other**-1\n",
    "\n",
    "    # e^(Value)\n",
    "    def exp(self):\n",
    "        out = Value(math.exp(self.data), (self, ), \"e^\")\n",
    "        \n",
    "        def _backward():\n",
    "            dout_dself = out.data\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # tanh(Value)\n",
    "    def tanh(self):\n",
    "        out = Value(math.tanh(self.data), (self, ) , \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = 1 - out.data**2\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    # Backprog stuff\n",
    "    def backward(self):\n",
    "        # Treat self as a root node\n",
    "        # dself/dself = 1.0, hence\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # Create a list of the sorted nodes of a digraph\n",
    "        # where the digraph is simply \n",
    "        # the graph of the expression of the Value objects leading up to self\n",
    "        topo_sorted = []\n",
    "        seen = set()\n",
    "\n",
    "        def explore(node):\n",
    "            if node not in seen:\n",
    "                seen.add(node)\n",
    "                for child in node._prev:\n",
    "                    explore(child)\n",
    "                topo_sorted.append(node)\n",
    "\n",
    "        # This updates our \"topo_sorted\" and \"seen\" local variables\n",
    "\n",
    "        explore(self) \n",
    "\n",
    "        # For every node in our reversed, topologically-sorted digraph,\n",
    "        # call its _backward behavior.\n",
    "        # reversed because we want to call the leaf nodes last,\n",
    "        # and the leaf nodes are the nodes with least in-degrees (topo-sort jargon) \n",
    "        for node in reversed(topo_sorted): \n",
    "            node._backward()\n",
    "\n",
    "\n",
    "\n",
    "    # Cosmetic stuff\n",
    "    def __repr__(self):\n",
    "        return f\"Value(\\\"{self.label}\\\", data={self.data})\"\n",
    "\n",
    "    def create_graph(self):\n",
    "        # Create a set of known edges (connections between Value objects) \n",
    "        # and known nodes (known Value objects)\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for child in node._prev:\n",
    "                        edges.add((child, node))\n",
    "                        build(child)\n",
    "\n",
    "            build(root)\n",
    "            return nodes, edges\n",
    "\n",
    "        def draw_dot(root):\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            dot = Digraph(format=\"svg\", graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "            # For each Value object, create a node for it\n",
    "            for node in nodes:\n",
    "                dot.node(name=str(id(node)), label=f\"{node.label} | data {node.data:.4f} | grad {node.grad:.4f}\", shape=\"record\")\n",
    "\n",
    "                # If this node was created by an operation, we must create a node for that too\n",
    "                # and then connect that operation node to the Value object node\n",
    "                if node._op:\n",
    "                    dot.node(name=str(id(node)) + node._op, label=node._op)\n",
    "                    dot.edge(str(id(node)) + node._op, str(id(node)))\n",
    "\n",
    "            # For each known connection between two Value objects\n",
    "            for child, parent in edges:\n",
    "                # Connect the child to the parent's operation node\n",
    "                dot.edge(str(id(child)), str(id(parent))+parent._op)\n",
    "\n",
    "            return dot\n",
    "\n",
    "        graph = draw_dot(self)\n",
    "        # graph.render(directory=\"graphviz_outs\")\n",
    "        return graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the forward pass again, do the backward, and (hopefully) get a better looking expression graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just building out the expression for a Neuron\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label=\"x1*w1\"\n",
    "x2w2 = x2*w2; x2w2.label=\"x2*w2\"\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1*w1 + x2*w2\"\n",
    "\n",
    "b = Value(6.8813735870195432, label=\"b\")\n",
    "\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "two_n = 2*n; two_n.label=\"2*n\"\n",
    "\n",
    "e = two_n.exp(); e.label = \"e^(2*n)\"\n",
    "\n",
    "e_minus_1 = e - 1; e_minus_1.label = \"e - 1\"\n",
    "\n",
    "e_plus_1 = e + 1; e_plus_1.label = \"e + 1\"\n",
    "\n",
    "o = e_minus_1 / e_plus_1; o.label = \"o\"\n",
    "\n",
    "o.backward()\n",
    "o.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome\n",
    "\n",
    "Now let's actually\n",
    "### Build out a MULTILAYERPERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs, layer_number=\"\", neuron_number=\"\"): \n",
    "\n",
    "        # Each Neuron has n weights\n",
    "\n",
    "        self.weights = [Value(random.uniform(-1, 1), label=f\"Weight {i} of Neuron {neuron_number} from Layer {layer_number}\") for i in range(n_inputs)]\n",
    "        # self.weights = [Value(1.0, label=f\"Weight {i} of Neuron {neuron_number} from Layer {layer_number}\") for i in range(n_inputs)]\n",
    "\n",
    "        # Each Neuron is also associated with a bias\n",
    "\n",
    "        self.bias = Value(random.uniform(-1, 1), label=f\"Bias of Neuron {neuron_number} from Layer {layer_number}\")\n",
    "        # self.bias = Value(2.0, label=f\"Bias of Neuron {neuron_number} from Layer {layer_number}\")\n",
    "\n",
    "        self.layer_number = layer_number\n",
    "        self.neuron_number = neuron_number\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # Given a set of n inputs,\n",
    "        # multiply the ith_input with the ith_weight for every ith input and weight,\n",
    "        # add together all of the ith_weighted_inputs,\n",
    "        # add the bias on top,\n",
    "        # and return the squashed sum\n",
    "        activation = sum((ith_weight*ith_input for ith_weight, ith_input in zip(self.weights, inputs)), self.bias)\n",
    "        out = activation.tanh()\n",
    "        out.label=f\"Output of Neuron {self.neuron_number} from Layer {self.layer_number}\"\n",
    "        # return activation\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = f\"Neuron({len(self.weights)}w=\"\n",
    "\n",
    "        ws = []\n",
    "        for w in self.weights:\n",
    "            ws.append(w)\n",
    "        ws = [f\"{w.data:.4f}\" for w in ws]\n",
    "        ws = \", \".join(ws)\n",
    "\n",
    "        output = output + f\"[{ws}]\" + f\", b={self.bias.data:.4f})\"\n",
    "\n",
    "        return output\n",
    "   \n",
    "class Layer:\n",
    "    def __init__(self, n_inputs_per_neuron, n_neurons, layer_number=\"\"):\n",
    "        self.neurons = [Neuron(n_inputs_per_neuron, layer_number=layer_number, neuron_number=i) for i in range(n_neurons)]\n",
    "        self.dimensionality = n_inputs_per_neuron\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # Given a set of n inputs,\n",
    "        # evaluate every Neuron in this Layer for those inputs\n",
    "        outs = [neuron(inputs) for neuron in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = f\"Layer(neurons={len(self.neurons)}, d={self.dimensionality}):\\n\"\n",
    "\n",
    "        ns = []\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            ns.append(f\"\\t{index}: {neuron}\")\n",
    "        ns = \"\\n\".join(ns)\n",
    "        \n",
    "        return output + ns\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_inputs_per_neuron, n_neurons_per_layer: list):\n",
    "\n",
    "        # Produces a list of [n_inputs_per_neuron, n_neurons_per_layer[0], n_neurons_per_layer[1], n_neurons_per_layer[2], ...]\n",
    "        sz = [n_inputs_per_neuron] + n_neurons_per_layer\n",
    "\n",
    "        n_layers = len(n_neurons_per_layer)\n",
    "        self.layers = [Layer(n_inputs_per_neuron=sz[i], n_neurons=sz[i+1], layer_number=i) for i in range(n_layers)]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Given an initial set of inputs,\n",
    "        # evaluate the first layer for the first set of inputs,\n",
    "        # use the output of that as the inputs for the second layer, evaluate it,\n",
    "        # use the output of the second layer evaluated by the outputs of the first layer evaluated by the inputs\n",
    "        # as\n",
    "        # the input for the evaluation of the third layer\n",
    "        # so on, so forth\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "\n",
    "        # Remember, if a Neuron is given less inputs than its weights,\n",
    "        # then it simply ignores the weights which have no inputs attribute to during the calculation of its activation\n",
    "        # However, for every Layer in an MLP, every Neuron of that Layer has\n",
    "        # the same number of inputs as\n",
    "        # the number of Neurons of the previous Layer\n",
    "        # Hence, this situation of \"Neurons-lacking inputs\" never actually happens\n",
    "\n",
    "        # Return the evaluation of the last Layer\n",
    "        return inputs\n",
    "    \n",
    "    def __repr__(self):\n",
    "        output = f\"MLP(layers={len(self.layers)})\\n\"\n",
    "        ls = []\n",
    "        for layer in self.layers:\n",
    "            ls.append(f\"{layer}\")\n",
    "        ls = \"\\n\".join(ls)\n",
    "        return output + ls\n",
    "    \n",
    "# Whenever a Neuron does not receive the \n",
    "# same amount of inputs as its amount of weights, \n",
    "# it just ignores the trailing weight(s) in the calculation of its activation\n",
    "def test0():\n",
    "    inputs = [1.0, 2.0, 3.0]\n",
    "    n = Neuron(4, 0, 0)\n",
    "    print(n)\n",
    "    \n",
    "    out = n(inputs) \n",
    "    return out\n",
    "\n",
    "# Building out a MLP\n",
    "def test1():\n",
    "    inputs = [3.0, 2.0, 1.0]\n",
    "    mlp = MLP(len(inputs), [4, 4, 1])\n",
    "\n",
    "    return mlp(inputs)\n",
    "\n",
    "out = test1()\n",
    "out.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is quite a lot\n",
    "\n",
    "Let me try to explain all that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start, once again, with the idea of a \n",
    "#### Neuron\n",
    "\n",
    "A Neuron consists of 4 key elements: Inputs, Weights, a Bias, and an Activation Function\n",
    "\n",
    "Every Input enters a Neuron through its assigned gate. In every gate, lies the assigned Weight of that Gate\n",
    "\n",
    "To cross the gate, the Input must multiply itself with the Weight that lies in that gate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process happens for every Input.\n",
    "\n",
    "All of these Inputs multiplied with their respective Weights - all of these _Weighted Inputs_ - make their way to the center of the Neuron\n",
    "\n",
    "Once all gathered, the Weighted Inputs become one (we sum all of the Weighted Inputs together)\n",
    "\n",
    "They become a single unit - **the Activation** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before leaving the Neuron, **the Activation** is treated to one more process: the Activation **Function**\n",
    "\n",
    "the Activation **Function** squishes and morphs **the Activation**, twisting it into a predifined format that other Neurons find acceptable\n",
    "\n",
    "This **Function** turns **the Activation** into an Output\n",
    "\n",
    "Then, and only then, does the Output leave the Neuron, ready to embark in its other adventure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now we have the notion of a Neuron. Let's now understand what a\n",
    "\n",
    "#### Layer of Neurons\n",
    "\n",
    "looks like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite simply, a Layer of Neurons is just a bunch of Neurons grouped together\n",
    "\n",
    "These Neurons each have their own Biases, Weights, and activation Functions\n",
    "\n",
    "But they all share one commonality: They all have the same number of Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does it mean to give a Layer a set of Inputs?\n",
    "\n",
    "Well, think of the Layer as a Mother with the power of Duplication\n",
    "\n",
    "The children are the Neurons, and they are hungry\n",
    "\n",
    "The Mother is given a set of Inputs, and it distributes those Inputs equally for each one of its children Neurons\n",
    "\n",
    "Each child receives the same set of Inputs, since the Mother has cloning powers\n",
    "\n",
    "The Neurons then eat the Inputs (in the fashion that we described above).\n",
    "\n",
    "Nurished, each Neuron works to produce their very own Output and, in appreciation for their Mother's care, they each give their Output to their Mother"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mother, then, finds herself with \"n\" Outputs, one for each of its Neurons\n",
    "\n",
    "And it returns those Outputs to whoever gave her the Inputs\n",
    "\n",
    "And can you guess who originally gave her (the Mother/Layer) those Inputs?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE MULTILAYERPERCEPTRON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually not as Evil as it sounds. If anything, the Multi-layer Perceptron, or MLP for short, is actually a very caring person, and quite concerned about its community\n",
    "\n",
    "In fact, it's actually easier to think of the MLP as an organization (perhaps a nonprofit one) that distributes food to those in need\n",
    "\n",
    "Amongst these people are our Mothers (or our Layers)\n",
    "\n",
    "The MLP is given an initial funding (the initial set of inputs), which it gives to Mother 1\n",
    "\n",
    "Mother 1, then, gives a unique copy of those Inputs to each of its children (the Neurons), generating a set of Outputs, which the Mother gives back to the MLP\n",
    "\n",
    "(See, the children are giving back to the community!)\n",
    "\n",
    "The MLP then reinvests those Inputs by giving it to Mother 2. Mother 2 generates its Outputs, which it gives back to the MLP\n",
    "\n",
    "The MLP then gives those Outputs as Inputs to Mother 3, which generates its outputs, and so soon, so forth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's just one \n",
    "#### Caveat\n",
    "To this process\n",
    "\n",
    "Remember how the only commonality between Neurons of the same Layer is their Number of Inputs?\n",
    "\n",
    "Well, so that actually matters in our process here.\n",
    "\n",
    "Let us say that Mother 1 has 10 Children, and that each of these Children have a total of 5 mouths (5 Inputs). An abomination, I know!\n",
    "\n",
    "Mother 1 will only be able to feed its Children (and, hence, generate its Outputs) if it is supplied with an Initial set of Inputs which contains 5 elements\n",
    "\n",
    "If the Input to the Mother contains 6 or 4 elements, we'll either have-left over food, or the Children will be malnourished, either scenario being unacceptable!\n",
    "\n",
    "Further still, if Mother 1 has 10 Children, that means that it generates an Output of 10 elements.\n",
    "\n",
    "Hence, if Mother 2 has 11 Children, this means that if we give Mother 1's Outputs as Inputs to Mother 2, Mother 2 will find herself with hungry children!\n",
    "\n",
    "So the MLP has a new job to perform: Make sure that its initial funding is enough to feed Mother 1's children, and make sure that Mother 1's Outputs are correspondent with the hungriness of Mother 2's children\n",
    "\n",
    "Or, in other words, the number of Initial Inputs to a MLP must equal to the number of Inputs that First-Layer-type Neurons have, and the Number of Neurons in any layer must equal the Number of Inputs that Neurons of the subsequent layer have __(such that the Number of Neurons of the First Layer equal the Number of Inputs of Second-Layer-type Neurons, and the Number of Neurons of the Second Layer equal the Number of Inputs of Third-Layer-type Neurons, and so on, so forth)__\n",
    "\n",
    "Please also note that Number of Neurons in any Layer does not have to equal to the Number of Neurons in the Previous Layer. All that matters is that the Number of Inputs for the Neurons of a Layer equal to the number of Neurons in the preceeding Layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "And that is essentially it! That is all you need to know to intuitivelly understand an MLP (without all of the adjusting weights, biases, and backprog stuff)\n",
    "\n",
    "So why don't we start tinkering with those latter concepts?\n",
    "\n",
    "Let's establish the following: we have a neural network which takes 3 inputs, and outputs a single value\n",
    "\n",
    "Let's write down four sets of Inputs, where each input contains 4 elements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then a set of what our desired outputs are.\n",
    "\n",
    "Since we have four sets of inputs, we should have a set of four desired outputs, one output for each set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we then define our Multi-layer Perceptron.\n",
    "\n",
    "Our Initial set of Inputs contains 3 elements.\n",
    "\n",
    "Hence, the Number of Inputs for First-Layer-type Neurons of our MLP should be 3\n",
    "\n",
    "Let's make a First Layer with 4 3-input Neurons, a Second Layer with 4 4-input Neurons, and a Final Layer with 1 4-input Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how easy that was?!\n",
    "\n",
    "Now we should just be able to do `mlp(xs[0])` and get a result\n",
    "\n",
    "That result, in this context, we'll interpret as the prediction for `ys[0]`\n",
    "\n",
    "That is, **evaluating our Set of Inputs with our Model yields a Prediction of our Set of Outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_pred = mlp(xs[0])\n",
    "print(y0_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, let's do this for every `xs` that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [mlp(xi) for xi in xs]\n",
    "for ygt, yout in zip(ys, ypred):\n",
    "    print(yout, \"Expected:\", ygt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see right now that our model is not performing very well.\n",
    "\n",
    "Indeed, when we expect it to output 1.0, it outputs approximately 0.2894, and when we expect it to output -1.0, it outputs approximately 0.0358\n",
    "\n",
    "(Please note that the numbers 0.2894 and 0.0358 are random, and will change if you run the code above again. Nevertheless, it is most likely that our model is not performing well for random weights and biases)\n",
    "\n",
    "In fact, **for every version of our model, we would like to qualify how well it is doing**\n",
    "\n",
    "For this purpose, we'll create what we call the **Loss Function**\n",
    "\n",
    "The **Loss Function** simply takes the sum of the squared __differences between the experimental outputs and the expected outputs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_terms = [(ygt - yout)**2 for ygt, yout in zip(ys, ypred)]\n",
    "for index, loss_term in enumerate(loss_terms):\n",
    "    loss_term.special_label = f\"Loss of Output {index}\"\n",
    "    loss_term.label = loss_term.label + loss_term.special_label\n",
    "loss_terms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction for our first set of inputs, namely `ypred[0]`, was close to the expected output, `ys[0]`\n",
    "\n",
    "Hence, it should come as no surprise that our loss for predicting approximately 0.7265 (`ypred[0]`) for the expected value of 1.0 (`ys[0]`) generates a low value of approximately 0.0748 (`(ys[0]-ypred[0])**2`)\n",
    "\n",
    "However, predicting 0.0358 for -1.0 is not good, and hence the corresponding loss value is much higher, at 1.0728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ideal way to compute the loss is by doing this...\n",
    "# loss = sum((ygt - yout)**2 for ygt, yout in zip(ys, ypred))\n",
    "# But I want to generate a labelled graph.\n",
    "# Hence,\n",
    "loss = loss_terms[0] + loss_terms[1] + loss_terms[2] + loss_terms[3]\n",
    "loss.label = \"loss\"; loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the cool thing about this loss: we can backpropagate through it, and graph it too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "loss.create_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you see, the cool thing now is that we are able to gather the grad of weights\n",
    "\n",
    "We are able to see how the weights of our Neural Network influence the Loss Function\n",
    "\n",
    "For instance, say that we want to decrease the final Loss value, and that a weight of a particular neuron in a particular layer is negative. \n",
    "\n",
    "This means that if we increase that particular weight of that particular neuron of that particular layer, we expect our loss function to get more negative\n",
    "\n",
    "Since we want to decrease our final loss value, increasing that particular weight should help us do that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `parameters()` method that should allow us to collect that weights and biases of a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs, layer_number=\"\", neuron_number=\"\"): \n",
    "\n",
    "        # Each Neuron has n weights\n",
    "\n",
    "        self.weights = [Value(random.uniform(-1, 1), label=f\"Weight {i} of Neuron {neuron_number} from Layer {layer_number}\") for i in range(n_inputs)]\n",
    "        # self.weights = [Value(1.0, label=f\"Weight {i} of Neuron {neuron_number} from Layer {layer_number}\") for i in range(n_inputs)]\n",
    "\n",
    "        # Each Neuron is also associated with a bias\n",
    "\n",
    "        self.bias = Value(random.uniform(-1, 1), label=f\"Bias of Neuron {neuron_number} from Layer {layer_number}\")\n",
    "        # self.bias = Value(2.0, label=f\"Bias of Neuron {neuron_number} from Layer {layer_number}\")\n",
    "\n",
    "        self.layer_number = layer_number\n",
    "        self.neuron_number = neuron_number\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # Given a set of n inputs,\n",
    "        # multiply the ith_input with the ith_weight for every ith input and weight,\n",
    "        # add together all of the ith_weighted_inputs,\n",
    "        # add the bias on top,\n",
    "        # and return the squashed sum\n",
    "        activation = sum((ith_weight*ith_input for ith_weight, ith_input in zip(self.weights, inputs)), self.bias)\n",
    "        out = activation.tanh()\n",
    "        out.label=f\"Output of Neuron {self.neuron_number} from Layer {self.layer_number}\"\n",
    "        # return activation\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = f\"Neuron({len(self.weights)}w=\"\n",
    "\n",
    "        ws = []\n",
    "        for w in self.weights:\n",
    "            ws.append(w)\n",
    "        ws = [f\"{w.data:.4f}\" for w in ws]\n",
    "        ws = \", \".join(ws)\n",
    "\n",
    "        output = output + f\"[{ws}]\" + f\", b={self.bias.data:.4f})\"\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, n_inputs_per_neuron, n_neurons, layer_number=\"\"):\n",
    "        self.neurons = [Neuron(n_inputs_per_neuron, layer_number=layer_number, neuron_number=i) for i in range(n_neurons)]\n",
    "        self.dimensionality = n_inputs_per_neuron\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # Given a set of n inputs,\n",
    "        # evaluate every Neuron in this Layer for those inputs\n",
    "        outs = [neuron(inputs) for neuron in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = f\"Layer(neurons={len(self.neurons)}, d={self.dimensionality}):\\n\"\n",
    "\n",
    "        ns = []\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            ns.append(f\"\\t{index}: {neuron}\")\n",
    "        ns = \"\\n\".join(ns)\n",
    "        \n",
    "        return output + ns\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_inputs_per_neuron, n_neurons_per_layer: list):\n",
    "\n",
    "        # Produces a list of [n_inputs_per_neuron, n_neurons_per_layer[0], n_neurons_per_layer[1], n_neurons_per_layer[2], ...]\n",
    "        sz = [n_inputs_per_neuron] + n_neurons_per_layer\n",
    "\n",
    "        n_layers = len(n_neurons_per_layer)\n",
    "        self.layers = [Layer(n_inputs_per_neuron=sz[i], n_neurons=sz[i+1], layer_number=i) for i in range(n_layers)]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Given an initial set of inputs,\n",
    "        # evaluate the first layer for the first set of inputs,\n",
    "        # use the output of that as the inputs for the second layer, evaluate it,\n",
    "        # use the output of the second layer evaluated by the outputs of the first layer evaluated by the inputs\n",
    "        # as\n",
    "        # the input for the evaluation of the third layer\n",
    "        # so on, so forth\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "\n",
    "        # Remember, if a Neuron is given less inputs than its weights,\n",
    "        # then it simply ignores the weights which have no inputs attribute to during the calculation of its activation\n",
    "        # However, for every Layer in an MLP, every Neuron of that Layer has\n",
    "        # the same number of inputs as\n",
    "        # the number of Neurons of the previous Layer\n",
    "        # Hence, this situation of \"Neurons-lacking inputs\" never actually happens\n",
    "\n",
    "        # Return the evaluation of the last Layer\n",
    "        return inputs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        output = f\"MLP(layers={len(self.layers)})\\n\"\n",
    "        ls = []\n",
    "        for layer in self.layers:\n",
    "            ls.append(f\"{layer}\")\n",
    "        ls = \"\\n\".join(ls)\n",
    "        return output + ls   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-init the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "mlp = MLP(n_inputs, [4, 4, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And collect its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of\", len(mlp.parameters()), \"parameters\")\n",
    "mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "ypred = [mlp(xi) for xi in xs]\n",
    "loss = sum((ygt - yout)**3 for ygt, yout in zip(ys, ypred))\n",
    "print(\"Current loss is\", loss.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the power to collect hte parameters of our MLP, let's change them\n",
    "\n",
    "Remember, all these parameters are just `Value` object which represent the Weights and Biases of Neurons\n",
    "\n",
    "Each one of them contains a `Value.data` and a `Value.grad`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that a Value has a negative grad\n",
    "\n",
    "Increasing that Value's data decreases the loss\n",
    "\n",
    "`Value.data += (-0.01) * Value.grad`\n",
    "\n",
    "Decreasing that Value's data increases the loss\n",
    "\n",
    "`Value.data += 0.01 * Value.grad`\n",
    "\n",
    "<br>\n",
    "\n",
    "Now say that a Value has a positive grad\n",
    "\n",
    "Increasing that Value's data increases the loss\n",
    "\n",
    "`Value.data += 0.01 * Value.grad`\n",
    "\n",
    "Decreasing that Value's data decreases the loss\n",
    "\n",
    "`Value.data += (-0.01) * Value.grad`\n",
    "\n",
    "<br>\n",
    "\n",
    "So it's clear from the above that, in order to tune our Neural Network to minimize our loss, we must\n",
    "\n",
    "`Value.data += (-0.01) * Value.grad`\n",
    "\n",
    "for every parameter\n",
    "\n",
    "It's just a matter of recognizing that \"**-**\" negative sign, and the nuance of that with `grad`\\`s being positive or negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this is thinking of `grad` as a vector that points in the direction of increasing the loss\n",
    "\n",
    "And, since we want to decrease the loss, we must go in the opposite direction of that vector - we must include the \"**-**\" negative sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "for p in mlp.parameters():\n",
    "    p.data += (-0.001) * p.grad\n",
    "ypred = [mlp(xi) for xi in xs]\n",
    "updated_loss = sum((ygt - yout)**3 for ygt, yout in zip(ys, ypred))\n",
    "print(\"Updated loss:\", updated_loss.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so clearly something didn't work.\n",
    "\n",
    "After a bit more of research, apparently this bug is caused because we must zero out the grad of our parameters before a backward pass\n",
    "\n",
    "Makes sense, we do not want the grads of previous backward passes accumulating in our current backward pass\n",
    "\n",
    "I'll paste here the code for everything, redefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from graphviz import Digraph\n",
    "import random as rand\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\", has_labels=False):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.has_labels = has_labels\n",
    "\n",
    "    def backward(self):\n",
    "        \n",
    "        topo_sorted = []\n",
    "        seen = set()\n",
    "\n",
    "        def explore(node):\n",
    "            if node not in seen:\n",
    "                seen.add(node)\n",
    "                for child in node._prev:\n",
    "                    explore(child)\n",
    "                topo_sorted.append(node)\n",
    "\n",
    "        # This changes the topo_sorted list to be a topological sort of our nodes \n",
    "        explore(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for node in reversed(topo_sorted):\n",
    "            node._backward()\n",
    "\n",
    "    def create_graph(self):\n",
    "\n",
    "        def trace(root):\n",
    "            nodes, edges = set(), set()\n",
    "\n",
    "            def build(node):\n",
    "                if node not in nodes:\n",
    "                    nodes.add(node)\n",
    "                    for child in node._prev:\n",
    "                        edges.add((child, node))\n",
    "                        build(child)\n",
    "\n",
    "            build(root)\n",
    "\n",
    "            return nodes, edges\n",
    "        \n",
    "        def graph(root):\n",
    "            nodes, edges = trace(root)\n",
    "\n",
    "            dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}, filename=\"my_graph\")\n",
    "\n",
    "            for node in nodes:\n",
    "                node_id = str(id(node))\n",
    "                dot.node(name=node_id, label=f\"{node.label} | data {node.data:.3f} | grad {node.grad:.3f}\", shape=\"record\")\n",
    "\n",
    "                if node._op:\n",
    "                    dot.node(name=(node_id+ node._op), label=node._op)\n",
    "                    dot.edge((node_id + node._op), node_id)\n",
    "\n",
    "            for child, parent in edges:\n",
    "                dot.edge(str(id(child)), (str(id(parent)) + parent._op))\n",
    "\n",
    "            return dot\n",
    "    \n",
    "        g = graph(self)\n",
    "        return g\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.3f})\" if self.label==\"\" else f\"Value(\\\"{self.label}\\\", data={self.data:.3f})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Value) else Value(data=other, label=f\"{other}\")\n",
    "\n",
    "        out = Value(data=(self.data + other.data), _children=(self, other), _op=\"+\")\n",
    "\n",
    "        l = f\"{self.label} + {other.label}\" if (self.label != \"\" and other.label != \"\" and self.has_labels) else \"\"\n",
    "        out.label = l \n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = 1\n",
    "            dout_dother = 1\n",
    "\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Value) else Value(data=other, label=f\"{other}\")\n",
    "\n",
    "        out = Value(data=(self.data * other.data), _children=(self, other), _op=\"*\")\n",
    "        \n",
    "        l = f\"({self.label})*{other.label}\" if (self.label != \"\" and other.label != \"\" and self.has_labels) else \"\"\n",
    "        out.label = l \n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = other.data\n",
    "            dout_dother = self.data\n",
    "\n",
    "            self.grad += out.grad * dout_dself\n",
    "            other.grad += out.grad * dout_dother\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "\n",
    "        assert isinstance(other, (int, float))\n",
    "\n",
    "        out = Value(data=(self.data**other), _children=(self, ), _op=f\"^{other}\")\n",
    "\n",
    "        l = f\"({self.label})^{other}\" if (self.label != \"\" and self.has_labels) else \"\"\n",
    "        out.label = l\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = other * self.data**(other - 1)\n",
    "            self.grad += out.grad * dout_dself\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def raise_euler(self):\n",
    "        out = Value(data=math.exp(self.data), _children=(self, ), _op=f\"e^{self.data}\")\n",
    "\n",
    "        l = f\"e^({self.label})\" if (self.label != \"\" and self.has_labels) else \"\"\n",
    "        out.label = l\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = math.exp(self.data)\n",
    "            self.grad += out.grad * dout_dself\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # tanh(x) = (e^(2x) - 1) / (e^(2x) + 1)\n",
    "    def tanh(self):\n",
    "        out = Value(data=math.tanh(self.data), _children=(self, ), _op=\"tanh\")\n",
    "\n",
    "        l = f\"tanh^({self.label})\" if (self.label != \"\" and self.has_labels) else \"\"\n",
    "        out.label = l\n",
    "\n",
    "        def _backward():\n",
    "            dout_dself = 1 - out.data**2\n",
    "            self.grad += out.grad * dout_dself\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "class Module:\n",
    "    pass\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_weights):\n",
    "        self.weights = [Value(rand.uniform(-1, 1)) for _ in range(n_weights)]\n",
    "        # self.weights = [Value(1) for _ in range(n_weights)]\n",
    "\n",
    "        self.bias = Value(rand.uniform(-1, 1))\n",
    "        # self.bias = Value(1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        activation = sum((input_i * weight_i for input_i, weight_i in zip(inputs, self.weights)), self.bias)\n",
    "        out = activation.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.bias] + self.weights\n",
    "\n",
    "    def __repr__(self):\n",
    "        name = \"\"\n",
    "        w = self.weights[0]\n",
    "        start = w.label.index(\"N\")\n",
    "        name = w.label[start:]\n",
    "        return f\"Neuron(w={[round(w.data, 3) for w in self.weights]}, b={round(self.bias.data, 3)}, name={name})\"\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_weights, n_neurons):\n",
    "        self.neurons = [Neuron(n_weights) for _ in range(n_neurons)]\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        outs = [n(inputs) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer({self.neurons})\"\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_weights_per_neuron, n_neurons_per_layer):\n",
    "        size = [n_weights_per_neuron] + n_neurons_per_layer\n",
    "        n_layers = len(n_neurons_per_layer)\n",
    "        self.layers = [Layer(n_weights=size[i], n_neurons=size[i+1]) for i in range(n_layers)]\n",
    "\n",
    "        # Use this alongside has_labels=True if you want something cursed\n",
    "        for layer_index, layer in enumerate(self.layers):\n",
    "            for neuron_index, neuron in enumerate(layer.neurons):\n",
    "                for weight_index, weight in enumerate(neuron.weights):\n",
    "                    weight.label = f\"W{weight_index}-N{neuron_index}-L{layer_index}\"\n",
    "                neuron.bias.label = f\"B-N{neuron_index}-L{layer_index}\"\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        for l in self.layers:\n",
    "            inputs = l(inputs)\n",
    "\n",
    "        return inputs \n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        ls = \"\"\n",
    "\n",
    "        for l in self.layers:\n",
    "            ls = ls + f\"{l}\\n\"\n",
    "\n",
    "        output = \"MLP\\n\" + ls\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP\n",
    "n_inputs = 3\n",
    "mlp = MLP(n_inputs, [4, 4, 1])\n",
    "\n",
    "# Print information about the MLP\n",
    "# print(mlp)\n",
    "# params = mlp.parameters()\n",
    "# print(f\"Total of {len(params)} parameters.\")\n",
    "# for p in mlp.parameters():\n",
    "#     print(p)\n",
    "\n",
    "# Define the training data-set\n",
    "inputs = [\n",
    "    [Value(2.0, label=\"1\"), Value(3.0, label=\"2\"), Value(-1.0, label=\"3\")],\n",
    "    [Value(3.0, label=\"4\"), Value(-1.0, label=\"5\"), Value(0.5, label=\"6\")],\n",
    "    [Value(0.5, label=\"7\"), Value(1.0, label=\"8\"), Value(1.0, label=\"9\")],\n",
    "    [Value(1.0, label=\"10\"), Value(1.0, label=\"11\"), Value(-1.0, label=\"12\")],\n",
    "]\n",
    "utopian_outputs = [Value(1.0, label=\"pred1\"), Value(-1.0, label=\"pred2\"), Value(-1.0, label=\"pred3\"), Value(1.0, label=\"pred4\")]\n",
    "before = [mlp(input_) for input_ in inputs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training loop\n",
    "for k in range(1000):\n",
    "\n",
    "    # Forward pass\n",
    "    # We have updated the parameters of our network.\n",
    "    # Hence, our actual_outputs must have changed\n",
    "    # And hence our loss must too have changed\n",
    "    actual_outputs = [mlp(input_) for input_ in inputs]\n",
    "    loss = sum((utopian_output - actual_output)**2 for utopian_output, actual_output in zip(utopian_outputs, actual_outputs))\n",
    "\n",
    "    # Zero-grad\n",
    "    # Basically flushing out all of the previous gradients\n",
    "    # Because they only tell us how to influence the loss using our previous parameters,\n",
    "    # and not utilizing the updated parameters of the previous loop\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    # Backward pass\n",
    "    # Update our info on how to influence the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Decrease the loss\n",
    "    for p in mlp.parameters():\n",
    "        p.data += (-0.01) * p.grad\n",
    "\n",
    "    # Display the decreased loss\n",
    "    # print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: [Value(data=0.379), Value(data=0.332), Value(data=0.635), Value(data=-0.150)]\n",
      "After training:  [Value(data=0.980), Value(data=-0.978), Value(data=-0.966), Value(data=0.968)]\n"
     ]
    }
   ],
   "source": [
    "# Display our network before and after training\n",
    "after = actual_outputs\n",
    "print(f\"Before training: {before}\")\n",
    "print(f\"After training:  {after}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
